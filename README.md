ğŸ§  Self-Hosted LLM Server
A secure, self-hosted AI server using Docker, FastAPI, Nginx, and Ollama with GPU acceleration.
Supports LLM inference, ChromaDB, Redis caching, and monitoring.

ğŸš€ Features
âœ” FastAPI-based AI API with GPU acceleration âš¡
âœ” Secure reverse proxy (Nginx) with HTTPS and authentication ğŸ”
âœ” Dockerized services (Ollama, Redis, ChromaDB, Grafana, FAISS, N8N) ğŸ³
âœ” Secrets management using Docker secrets ğŸ›¡ï¸
âœ” Supports both CPU and GPU inference ğŸ®
âœ” Remote access via a hosted web server ğŸŒ
