🧠 Self-Hosted LLM Server
A secure, self-hosted AI server using Docker, FastAPI, Nginx, and Ollama with GPU acceleration.
Supports LLM inference, ChromaDB, Redis caching, and monitoring.

🚀 Features
✔ FastAPI-based AI API with GPU acceleration ⚡
✔ Secure reverse proxy (Nginx) with HTTPS and authentication 🔐
✔ Dockerized services (Ollama, Redis, ChromaDB, Grafana, FAISS, N8N) 🐳
✔ Secrets management using Docker secrets 🛡️
✔ Supports both CPU and GPU inference 🎮
✔ Remote access via a hosted web server 🌍
