ğŸ§  Self-Hosted LLM Server
A secure, self-hosted AI server using Docker, FastAPI, Nginx, and Ollama with GPU acceleration.
Supports LLM inference, ChromaDB, Redis caching, and monitoring.

ğŸš€ Features
âœ” FastAPI-based AI API with GPU acceleration âš¡
âœ” Secure reverse proxy (Nginx) with HTTPS and authentication ğŸ”
âœ” Dockerized services (Ollama, Redis, ChromaDB, Grafana, FAISS, N8N) ğŸ³
âœ” Secrets management using Docker secrets ğŸ›¡ï¸
âœ” Supports both CPU and GPU inference ğŸ®
âœ” Remote access via a hosted web server ğŸŒ

ğŸ”§ Setup & Installation
1ï¸âƒ£ Prerequisites
  ğŸ³ Docker & Docker Compose installed (Get Docker)
  âœ… Nvidia GPU (optional, but recommended)
  ğŸ” Domain name + SSL certificate (for secure remote access)
2ï¸âƒ£ Clone the Repository
3ï¸âƒ£ Configure Environment Variables
4ï¸âƒ£ Set Up Secrets
5ï¸âƒ£ Start Services
ğŸ¯ Future Improvements
ğŸ”¹ Add OAuth authentication for the LLM API
ğŸ”¹ Implement rate limiting to prevent abuse
ğŸ”¹ Improve logging & monitoring
ğŸ“œ License
MIT License. Feel free to use and modify! ğŸ˜ŠğŸš€
