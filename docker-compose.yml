version: "3.9"

volumes:
  ollama_storage:
  redis_data:
  grafana_data:
  chromadb_data:
  faiss_data:
  n8n_storage:

networks:
  demo:
    driver: bridge
    internal: true  # Keep all services internal by default

  web:
    driver: bridge  # Only Nginx proxy is exposed

secrets:
  redis_password:
    file: ./secrets/redis_password.txt
  grafana_password:
    file: ./secrets/grafana_password.txt
  chroma_password:
    file: ./secrets/chroma_password.txt
  llm_api_key:
    file: ./secrets/llm_api_key.txt  # Secure API key for LLM server

x-resource-configs: &resource-configs
  high-resources: &high-resources
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 32G
        reservations:
          cpus: '4.0'
          memory: 16G

  medium-resources: &medium-resources
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G

  light-resources: &light-resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G

services:
  db_backup:
    image: postgres:latest
    command: pg_dump -h db -U user -F c ai_db > /backups/backup.sql
    volumes:
      - ./backups:/backups
    environment:
      PGPASSWORD: password

  nginx-proxy:
    image: nginx:latest
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"  # Enable HTTPS
    networks:
      - web
      - demo
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/certs:/etc/nginx/certs
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - ollama-gpu
    security_opt:
      - no-new-privileges:true
    environment:
      - OLLAMA_HOST=ollama-gpu:11434  # Proxy to LLM server

  ollama-gpu:
    <<: [*high-resources]
    image: ollama/ollama:latest
    container_name: ollama-gpu
    restart: always
    networks:
      - demo
    volumes:
      - ollama_storage:/root/.ollama
    environment:    # âœ… AI Performance Optimizations
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_CUDA_MALLOC=20GB
      - OLLAMA_CUDA_STREAM_BATCH_SIZE=32
      - OLLAMA_QUANTIZATION=q4_K_M
      - OLLAMA_API_KEY_FILE=/run/secrets/llm_api_key  # Require API key for access
      - OMP_NUM_THREADS=16  # Optimizes CPU threading
      - TF_ENABLE_ONEDNN_OPTS=1  # Boosts TensorFlow performance
      - TORCH_CUDA_ALLOC_CONF=expandable_segments:True  # Prevents CUDA fragmentation
    secrets:
      - llm_api_key
    security_opt:
      - no-new-privileges:true
    read_only: true
    healthcheck:
      test: ["CMD", "curl", "-H", "Authorization: Bearer $(cat /run/secrets/llm_api_key)", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      retries: 3


  redis:
    <<: *light-resources
    image: redislabs/redisai
    command: redis-server --loadmodule /usr/lib/redis/modules/redisai.so
    networks:
      - demo
    secrets:
      - redis_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  chroma:
    <<: *high-resources
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chroma
    restart: unless-stopped
    networks:
      - demo
    secrets:
      - chroma_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  faiss:
    <<: *medium-resources
    image: pytorch/pytorch:latest
    container_name: faiss
    restart: unless-stopped
    networks:
      - demo
    security_opt:
      - no-new-privileges:true
    read_only: true

  grafana:
    <<: *light-resources
    image: grafana/grafana:latest
    networks:
      - demo
    ports:
      - "127.0.0.1:3001:3000"
    secrets:
      - grafana_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  cadvisor:
    <<: *light-resources
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    privileged: true
    networks:
      - demo
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    read_only: true
