version: "3.9"

volumes:
  ollama_storage:
  redis_data:
  grafana_data:
  chromadb_data:
  faiss_data:
  n8n_storage:
  whisper_data:
  piper_data:

networks:
  demo:
    driver: bridge
    internal: true  # Keep all services internal by default

  web:
    driver: bridge  # Only Nginx proxy is exposed

secrets:
  redis_password:
    file: ./secrets/redis_password.txt
  grafana_password:
    file: ./secrets/grafana_password.txt
  chroma_password:
    file: ./secrets/chroma_password.txt
  llm_api_key:
    file: ./secrets/llm_api_key.txt  # Secure API key for LLM server

x-resource-configs: &resource-configs
  high-resources: &high-resources
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 32G
        reservations:
          cpus: '4.0'
          memory: 16G

  medium-resources: &medium-resources
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G

  light-resources: &light-resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G

services:
  db_backup:
    image: postgres:latest
    command: pg_dump -h db -U user -F c ai_db > /backups/backup.sql
    volumes:
      - ./backups:/backups
    environment:
      PGPASSWORD: password

  nginx-proxy:
    image: nginx:latest
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"  # Enable HTTPS
    networks:
      - web
      - demo
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/certs:/etc/nginx/certs
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - ollama-gpu
    security_opt:
      - no-new-privileges:true
    environment:
      - OLLAMA_HOST=ollama-gpu:11434 # Proxy to LLM server

  # üè† Home Assistant (Now Handles Whisper STT & Piper TTS)
  home-assistant:
    <<: *medium-resources
    image: homeassistant/home-assistant:latest
    container_name: home-assistant
    restart: always
    networks:
      - demo
    volumes:
      - ./home-assistant:/config
    ports:
      - "8123:8123"

  # üöÄ Ollama (Primary LLM Engine)
  ollama-gpu:
    <<: *high-resources
    image: ollama/ollama:latest
    container_name: ollama-gpu
    restart: always
    networks:
      - demo
    volumes:
      - ollama_storage:/root/.ollama
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_CUDA_MALLOC=20GB # drop this to Prevent overflow. currently keeps 4GB free on a 24GB rtx
      - OLLAMA_USE_CPU_FALLBACK=True  # Enables CPU execution when GPU is busy
      - OLLAMA_CUDA_STREAM_BATCH_SIZE=64 # could go to 32 to decrease batch size if there are any issues
      - OLLAMA_MAX_WORKERS=4  # Allows up to 4 AI requests at the same time
      - OLLAMA_QUEUE_SIZE=10  # Ensures requests wait in queue instead of failing
      - OLLAMA_QUANTIZATION=q4_K_M # Best balance of size & accuracy
      - OLLAMA_API_KEY_FILE=/run/secrets/llm_api_key  # Require API key for access
      - OMP_NUM_THREADS=16  # Optimizes CPU threading
      - TF_ENABLE_ONEDNN_OPTS=1  # Boosts TensorFlow performance
      - TORCH_CUDA_ALLOC_CONF=expandable_segments:True  # Prevents CUDA fragmentation
    secrets:
      - llm_api_key
    security_opt:
      - no-new-privileges:true
    read_only: true
    healthcheck:
      test: ["CMD", "curl", "-H", "Authorization: Bearer $(cat /run/secrets/llm_api_key)", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu, utility, compute]

  # üîÅ RedisAI (For AI Task Queues)
  redis:
    <<: *medium-resources
    image: redislabs/redisai
    command: redis-server --loadmodule /usr/lib/redis/modules/redisai.so
    networks:
      - demo
    secrets:
      - redis_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üìú Celery Worker (Processes AI Jobs)
  celery-worker:
    <<: *medium-resources
    image: my-ai-worker:latest
    container_name: celery-worker
    restart: always
    depends_on:
      - redis
      - ollama-gpu
    networks:
      - demo
    environment:
      - REDIS_URL=redis://redis:6379
      - MAX_CONCURRENT_TASKS=3

  # üîç ChromaDB (For AI Vector Search)
  chroma:
    <<: *medium-resources
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chroma
    restart: unless-stopped
    networks:
      - demo
    secrets:
      - chroma_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  # ‚ö° FAISS (Optimized Vector Search)
  faiss:
    <<: *medium-resources
    image: pytorch/pytorch:latest
    container_name: faiss
    restart: unless-stopped
    networks:
      - demo
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üìä Grafana (Monitoring)
  grafana:
    <<: *light-resources
    image: grafana/grafana:latest
    networks:
      - demo
    ports:
      - "127.0.0.1:3001:3000"
    secrets:
      - grafana_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üîé cAdvisor (System Monitoring)
  cadvisor:
    <<: *light-resources
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    privileged: true
    networks:
      - demo
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    read_only: true
