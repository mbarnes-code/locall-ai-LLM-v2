version: "3.9"

volumes:
  ollama_storage:
  redis_data:
  grafana_data:
  milvus_data:
  faiss_data:
  n8n_storage:
  whisper_data:
  piper_data:

networks:
  demo:
    driver: bridge
    internal: true  # Keep all services internal by default

  web:
    driver: bridge  # Only Nginx proxy is exposed

secrets:
  redis_password:
    file: ./secrets/redis_password.txt
  grafana_password:
    file: ./secrets/grafana_password.txt
  milvus_password:
    file: ./secrets/milvus_password.txt
  llm_api_key:
    file: ./secrets/llm_api_key.txt  # Secure API key for LLM server

x-resource-configs: &resource-configs
  high-resources: &high-resources
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 32G
        reservations:
          cpus: '4.0'
          memory: 16G

  medium-resources: &medium-resources
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          cpus: '2.0'
          memory: 8G

  light-resources: &light-resources
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G

services:
  db_backup:
  image: ubuntu
  container_name: milvus-redis-backup
  restart: always
  volumes:
    - ./backups:/backups
  networks:
    - demo
  command: bash -c "
    tar -czvf /backups/milvus_backup.tar.gz /var/lib/milvus &&
    redis-cli --rdb /backups/redis_backup.rdb"


  nginx-proxy: #  expose AI services via HTTPS for remote/mobile access. (remove if you keep it strictly local)
    image: nginx:latest
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"  # Enable HTTPS
    networks:
      - web
      - demo
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./nginx/certs:/etc/nginx/certs
      - ./nginx/logs:/var/log/nginx
    depends_on:
      - ollama-gpu
    security_opt:
      - no-new-privileges:true
    environment:
      - OLLAMA_HOST=ollama-gpu:11434 # Proxy to LLM server

  # üè† Home Assistant (Now Handles Whisper STT & Piper TTS)
  home-assistant:
    <<: *medium-resources
    image: homeassistant/home-assistant:latest
    container_name: home-assistant
    restart: always
    networks:
      - demo
    volumes:
      - ./home-assistant:/config
    ports:
      - "8123:8123"

  #  ollama-gpu:
    <<: *high-resources
    build:
      context: ./ollama  # Use custom Dockerfile
    container_name: ollama-gpu
    restart: always
    networks:
      - demo
    volumes:
      - ollama_storage:/root/.ollama
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_CUDA_MALLOC=20GB  # Prevents overflow. Keeps 4GB free on a 24GB RTX
      - OLLAMA_USE_CPU_FALLBACK=True  # Enables CPU execution when GPU is busy
      - OLLAMA_CUDA_STREAM_BATCH_SIZE=64  # Allows better GPU parallelism. Adjust to 32 if needed
      - OLLAMA_MAX_WORKERS=4  # Allows up to 4 AI requests at the same time
      - OLLAMA_QUEUE_SIZE=10  # Ensures requests wait in queue instead of failing
      - OLLAMA_QUANTIZATION=q4_K_M  # Best balance of size & accuracy
      - OLLAMA_API_KEY_FILE=/run/secrets/llm_api_key  # Require API key for access
    secrets:
      - llm_api_key
    security_opt:
      - no-new-privileges:true
    read_only: true
    healthcheck:
      test: ["CMD", "curl", "-H", "Authorization: Bearer $(cat /run/secrets/llm_api_key)", "-f", "http://localhost:11434/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu, utility, compute]

  # üîÑ Apache Airflow (ETL for AI Training)
  airflow:
    <<: *medium-resources
    image: apache/airflow:latest
    container_name: airflow
    restart: always
    networks:
      - demo
    ports:
      - "8080:8080"  # Airflow Web UI
    volumes:
      - ./airflow/dags:/opt/airflow/dags  # Store AI training pipelines
      - ./airflow/logs:/opt/airflow/logs  # Store logs
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@postgres:5432/airflow
    depends_on:
      - postgres


  # üîÅ RedisAI (For AI Task Queues)
  redis:
    <<: *medium-resources
    image: redislabs/redisai
    command: redis-server --loadmodule /usr/lib/redis/modules/redisai.so
    networks:
      - demo
    secrets:
      - redis_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üìú Celery Worker (Processes AI Jobs)
  celery-worker:
    <<: *medium-resources
    build:
      context: ./celery-worker  # Use custom Dockerfile
    container_name: celery-worker
    restart: always
    depends_on:
      - redis
      - ollama-gpu
    networks:
      - demo
    environment:
      - REDIS_URL=redis://redis:6379
      - MAX_CONCURRENT_TASKS=3

  milvus:
    <<: *medium-resources  # Instead of high-resources. increase to high if RAM resources are availible 
    image: milvusdb/milvus:latest
    container_name: milvus
    restart: unless-stopped
    networks:
      - demo
    volumes:
      - milvus_data:/var/lib/milvus
    environment:
      - MILVUS_DB=postgres
      - MILVUS_POSTGRES_HOST=postgres
      - MILVUS_POSTGRES_PORT=5432
      - MILVUS_POSTGRES_USER=user
      - MILVUS_POSTGRES_PASSWORD=password
    security_opt:
      - no-new-privileges:true
    read_only: false  # Milvus needs write access

  # üóÑÔ∏è PostgreSQL (Milvus Metadata Database)
  postgres:
    <<: *medium-resources
    image: postgres:latest
    container_name: postgres
    restart: always
    networks:
      - demo
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=milvus
    volumes:
      - ./pg_data:/var/lib/postgresql/data


  # ‚ö° FAISS (Optimized Vector Search)
  faiss:
    <<: *medium-resources
    image: pytorch/pytorch:latest
    container_name: faiss
    restart: unless-stopped
    networks:
      - demo
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üìä Grafana (Monitoring)
  grafana:
    <<: *light-resources
    image: grafana/grafana:latest
    networks:
      - demo
    ports:
      - "127.0.0.1:3001:3000"
    secrets:
      - grafana_password
    security_opt:
      - no-new-privileges:true
    read_only: true

  # üîé cAdvisor (System Monitoring)
  cadvisor:
    <<: *light-resources
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: cadvisor
    privileged: true
    networks:
      - demo
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    read_only: true
